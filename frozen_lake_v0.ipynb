{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frozen-lake-v0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkLYQpB9WFP71MDb6ZsOCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthick47v2/FoodMenu/blob/main/frozen_lake_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gym.openai.com/envs/FrozenLake-v0/\n",
        "\n",
        "# States 4\n",
        "\n",
        "# 1 - S - Starting point - Safe\n",
        "# 2 - F - Forzen lake - Safe \n",
        "# 3 - H - Hole - Unsafe\n",
        "# 4 - G - Goal - Safe"
      ],
      "metadata": {
        "id": "qRk8JULZ4jz2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "yvwEf17T4BBr"
      },
      "outputs": [],
      "source": [
        "# import \n",
        "import gym          # for getting env\n",
        "import random       # generate random path (number) - exploration\n",
        "import numpy as np  # q-table\n",
        "import time         # sleep\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate frozen-lake-v0 environment\n",
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "metadata": {
        "id": "KM_u301X5fW-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate Q-table\n",
        "\n",
        "# no of rows = no of States\n",
        "# no of cols = no of Actions\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "num_states = env.observation_space.n\n",
        "\n",
        "# initialize Q-table with all zeros coz, at start we don't have any prior q values\n",
        "\n",
        "# q_table = np.zeros((num_states, num_actions))"
      ],
      "metadata": {
        "id": "5VPCs8I05qQf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "## We don't want to train forever, so we need to have some boudnaries\n",
        "####\n",
        "#  num of episodes for training\n",
        "num_episodes = 10000\n",
        "# num of steps agents takes max per episode\n",
        "max_steps_per_episode = 100"
      ],
      "metadata": {
        "id": "S25r7plI6dTK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(l_rate=0.1, d_rate=0.99, min_exp_rate=0.01, exp_decay_rate=0.01):\n",
        "  # initialize Q-table with all zeros coz, at start we don't have any prior q values\n",
        "\n",
        "  q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "\n",
        "  # initialize Q-learning params\n",
        "  # how much agent adopts from new q-value AKA Learning rate - alpha\n",
        "  learning_rate = l_rate\n",
        "  # Make sure agent focus on maximizing rewards for near future AKA discount rate - gamma\n",
        "  discount_rate = d_rate\n",
        "\n",
        "  ####\n",
        "  ## Exploration Vs Exploitation\n",
        "  ###\n",
        "\n",
        "  # start with max exploration rate so that agent can learn the env then increase the exploitation rate\n",
        "\n",
        "  exploration_rate = 1\n",
        "  max_exploration_rate = 1\n",
        "  min_exploration_rate = min_exp_rate\n",
        "  exploration_decay_rate = exp_decay_rate ### 0.001-------------------------------------------------\n",
        "\n",
        "  # store rewards for each episodes\n",
        "  rewards_all_episodes = []\n",
        "\n",
        "  # here is our training starts\n",
        "  # for all episodes \n",
        "  for episode in range(num_episodes):\n",
        "    # initialize episodic variables\n",
        "    # rewards for episode\n",
        "    rewards = 0\n",
        "    # chekc if episode done\n",
        "    done = False\n",
        "    # MOST IMPORTANT ::::: RESET STATE for each episode \n",
        "    state = env.reset()\n",
        "\n",
        "    # now lets see actions for each timestep \n",
        "    for step in range(max_steps_per_episode):\n",
        "      # Explore vs Exploit\n",
        "      exploration_rate_threshold = random.uniform(0,1)\n",
        "      action = 0\n",
        "      # if current exploration rate is less than threshold then explore, else exploit\n",
        "      if exploration_rate_threshold <= exploration_rate:\n",
        "        #explore\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        #exploit\n",
        "        action = np.argmax(q_table[state, :])\n",
        "      # print(exploration_rate)\n",
        "      # now we have chosen the action, lets see the results of action\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "      # now update the Q-value on table\n",
        "      ## FORMULA = q(s,a) = (1 - alpha) * q(s',a') (old q value) + alpha * (reward for current action + discounted max q value for next state-action pair)\n",
        "      q_table[state,action] = (1 - learning_rate) * q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state,:]))\n",
        "      # if(reward == 1):\n",
        "      #   print(reward)\n",
        "      # change to new state\n",
        "      state = new_state\n",
        "      # add reward\n",
        "      rewards += reward\n",
        "\n",
        "      # check if episode is done (achieved goal or fell on hole)\n",
        "      if done:\n",
        "        # if so, finish episode\n",
        "        break\n",
        "\n",
        "    # if not, before going to next episode decay the expolartion rate --- exploration rate will exponentially decay\n",
        "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
        "\n",
        "    # append rewards to list for each episodes\n",
        "    rewards_all_episodes.append(rewards)\n",
        "\n",
        "  return rewards_all_episodes, q_table"
      ],
      "metadata": {
        "id": "Exo_MlyA8oF_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all_episodes, q_table = train_agent(l_rate=(0.1), d_rate=0.99, min_exp_rate=0.01, exp_decay_rate=0.001)\n",
        "\n",
        "# after training finished, lets see results -- avg\n",
        "rewards_per_1000_episodes = np.split(np.array(rewards_all_episodes), 10)\n",
        "count = 1000\n",
        "print(\"l_rate = \", str(0.1), \"d_rate = \", str(0.99), \"min_exp_rate = \", str(0.01), \"exp_decay_rate = \", str(0.01))\n",
        "\n",
        "for i in rewards_per_1000_episodes:\n",
        "  print(count, \" : \", str(sum(i) / 1000))\n",
        "  count += 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttv8AcR6GZ68",
        "outputId": "5a39801e-3f77-4358-c1c9-02db0a3820e5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l_rate =  0.1 d_rate =  0.99 min_exp_rate =  0.01 exp_decay_rate =  0.01\n",
            "1000  :  0.046\n",
            "2000  :  0.187\n",
            "3000  :  0.423\n",
            "4000  :  0.564\n",
            "5000  :  0.653\n",
            "6000  :  0.658\n",
            "7000  :  0.675\n",
            "8000  :  0.681\n",
            "9000  :  0.68\n",
            "10000  :  0.709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets put the agent to test\n",
        "\n",
        "for episode in range(1,4):\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "\n",
        "  print(\"EPISODE \", episode)\n",
        "  time.sleep(1)\n",
        "\n",
        "  for step in range(max_steps_per_episode):\n",
        "    clear_output(wait=True)\n",
        "    # render \n",
        "    env.render() \n",
        "    time.sleep(0.3)\n",
        "\n",
        "    # do action\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    # result\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      clear_output(wait=True)\n",
        "      env.render()\n",
        "      \n",
        "      if reward == 1:\n",
        "        print(\"GOAL\")\n",
        "      else:\n",
        "        print(\"FAILED\")\n",
        "      time.sleep(3)\n",
        "      clear_output(wait=True)\n",
        "      break;\n",
        "    \n",
        "    state = new_state\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiGrPGewM3iI",
        "outputId": "3eea3fc9-86f2-4aa5-e3c9-b481d83e23ee"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "GOAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AHxgotJHl_wg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}